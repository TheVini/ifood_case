# Databricks notebook source
"""
M√≥dulo de Transforma√ß√£o de Dados - Raw Layer

Este m√≥dulo √© respons√°vel por ler os arquivos Parquet da Landing Zone,
aplicar transforma√ß√µes e valida√ß√µes, e persistir os dados na camada Raw
do Data Lake utilizando formato Delta Lake.

Autor: Vinicius
Data: 2025
"""

# COMMAND ----------

# Necess√°rio pois s√≥ h√° m√°quinas serverless no Databricks
%pip install loguru

# COMMAND ----------

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import (
    StructType, StructField, LongType, TimestampType, 
    DoubleType, StringType, DateType
)
from pyspark.sql.functions import col, to_date, lit
from datetime import datetime
from dateutil.relativedelta import relativedelta
from loguru import logger
from typing import Tuple, List, Set

import re
import pytz

# COMMAND ----------

# Configura√ß√£o da sess√£o Spark
spark = SparkSession.builder \
    .appName("iFood-NYC-Taxi-Pipeline") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Data atual no timezone de S√£o Paulo
now = datetime.now(tz=pytz.timezone("America/Sao_Paulo")).date()

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS IDENTIFIER(:catalogo || '.' || :schema || '.' || :table) (
# MAGIC   VendorID BIGINT 
# MAGIC     COMMENT 'C√≥digo indicando o provedor TPEP que forneceu o registro. 1=Creative Mobile Technologies LLC, 2=Curb Mobility LLC, 6=Myle Technologies Inc, 7=Helix',
# MAGIC   tpep_pickup_datetime TIMESTAMP 
# MAGIC     COMMENT 'Data e hora em que o tax√≠metro foi acionado',
# MAGIC   tpep_dropoff_datetime TIMESTAMP 
# MAGIC     COMMENT 'Data e hora em que o tax√≠metro foi desligado',
# MAGIC   passenger_count DOUBLE 
# MAGIC     COMMENT 'N√∫mero de passageiros no ve√≠culo (informado pelo motorista)',
# MAGIC   trip_distance DOUBLE 
# MAGIC     COMMENT 'Dist√¢ncia percorrida em milhas reportada pelo tax√≠metro',
# MAGIC   RatecodeID DOUBLE 
# MAGIC     COMMENT 'C√≥digo de tarifa final em vigor no fim da corrida. 1=Standard rate, 2=JFK, 3=Newark, 4=Nassau or Westchester, 5=Negotiated fare, 6=Group ride, 99=Null/unknown',
# MAGIC   store_and_fwd_flag STRING 
# MAGIC     COMMENT 'Flag indicando se o registro foi armazenado na mem√≥ria do ve√≠culo antes de enviar ao vendor. Y=store and forward trip, N=not a store and forward trip',
# MAGIC   PULocationID BIGINT 
# MAGIC     COMMENT 'TLC Taxi Zone onde o tax√≠metro foi acionado (pickup location)',
# MAGIC   DOLocationID BIGINT 
# MAGIC     COMMENT 'TLC Taxi Zone onde o tax√≠metro foi desligado (dropoff location)',
# MAGIC   payment_type BIGINT 
# MAGIC     COMMENT 'C√≥digo num√©rico indicando como o passageiro pagou. 0=Flex Fare trip, 1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided trip',
# MAGIC   fare_amount DOUBLE 
# MAGIC     COMMENT 'Tarifa tempo-dist√¢ncia calculada pelo tax√≠metro. Ver https://www.nyc.gov/site/tlc/passengers/taxi-fare.page',
# MAGIC   extra DOUBLE 
# MAGIC     COMMENT 'Extras e sobretaxas diversos (ex: rush hour, overnight charges)',
# MAGIC   mta_tax DOUBLE 
# MAGIC     COMMENT 'Imposto MTA acionado automaticamente com base na tarifa em uso ($0.50)',
# MAGIC   tip_amount DOUBLE 
# MAGIC     COMMENT 'Valor da gorjeta. Campo populado automaticamente para gorjetas em cart√£o de cr√©dito. Gorjetas em dinheiro N√ÉO s√£o inclu√≠das',
# MAGIC   tolls_amount DOUBLE 
# MAGIC     COMMENT 'Valor total de ped√°gios pagos na corrida',
# MAGIC   improvement_surcharge DOUBLE 
# MAGIC     COMMENT 'Sobretaxa de melhoria avaliada na bandeirada. Come√ßou a ser cobrada em 2015 ($0.30)',
# MAGIC   total_amount DOUBLE 
# MAGIC     COMMENT 'Valor total cobrado dos passageiros. N√ÉO inclui gorjetas em dinheiro',
# MAGIC   congestion_surcharge DOUBLE 
# MAGIC     COMMENT 'Valor total coletado na corrida para sobretaxa de congestionamento do Estado de NY',
# MAGIC   airport_fee DOUBLE 
# MAGIC     COMMENT 'Taxa cobrada apenas para pickups nos aeroportos LaGuardia e JFK ($1.25)',
# MAGIC   date_reference DATE 
# MAGIC     COMMENT 'Data de refer√™ncia do m√™s dos dados (formato YYYY-MM-01) - campo t√©cnico para particionamento',
# MAGIC   insertion_date DATE 
# MAGIC     COMMENT 'Data de inser√ß√£o dos dados na tabela - campo t√©cnico para auditoria'
# MAGIC )
# MAGIC COMMENT 'Tabela com os dados brutos de TAXI de NY.'
# MAGIC CLUSTER BY (date_reference, VendorID)
# MAGIC LOCATION 's3://datalake-ifood/raw_layer/tb_taxi_data_api' ;

# COMMAND ----------

def validar_datas(start_date: str, end_date: str) -> Tuple[datetime, datetime]:
    """
    Valida os valores de start_date e end_date.
    
    Verifica se as datas est√£o no formato correto (YYYY-MM), se a data inicial
    n√£o √© posterior √† data final e se n√£o s√£o datas futuras.
    
    Args:
        start_date (str): Data de in√≠cio no formato YYYY-MM (ex: "2023-01")
        end_date (str): Data de fim no formato YYYY-MM (ex: "2023-05")
    
    Returns:
        Tuple[datetime, datetime]: Tupla contendo (data_inicio, data_fim) convertidas
                                   para objetos datetime
    
    Raises:
        ValueError: Se o formato das datas for inv√°lido, se data_inicio > data_fim,
                   ou se as datas estiverem no futuro
    
    Examples:
        >>> validar_datas("2023-01", "2023-05")
        (datetime(2023, 1, 1, 0, 0), datetime(2023, 5, 1, 0, 0))
        
        >>> validar_datas("2023-13", "2023-05")  # M√™s inv√°lido
        ValueError: data_inicio inv√°lida: '2023-13'. Formato esperado: YYYY-MM
    """
    # Regex para garantir o formato correto YYYY-MM
    padrao = r"^\d{4}-(0[1-9]|1[0-2])$"
    
    if not re.match(padrao, start_date):
        raise ValueError(f"data_inicio inv√°lida: '{start_date}'. Formato esperado: YYYY-MM")
    if not re.match(padrao, end_date):
        raise ValueError(f"data_fim inv√°lida: '{end_date}'. Formato esperado: YYYY-MM")

    # Converter para datetime
    data_inicio = datetime.strptime(start_date, "%Y-%m")
    data_fim = datetime.strptime(end_date, "%Y-%m")

    # Verificar se in√≠cio vem antes do fim
    if data_inicio > data_fim:
        raise ValueError(f"data_inicio ({start_date}) n√£o pode ser posterior √† data_fim ({end_date}).")

    # Bloquear datas futuras
    hoje = datetime.today()
    if data_inicio > hoje or data_fim > hoje:
        raise ValueError("Datas n√£o podem estar no futuro.")

    return data_inicio, data_fim

# COMMAND ----------

def generate_month_range(start_date: str, end_date: str) -> List[Tuple[int, int]]:
    """
    Gera uma lista de tuplas (ano, m√™s) entre start_date e end_date (inclusive).
    
    √ötil para iterar sobre todos os meses em um intervalo de datas para processar
    dados mensais sequencialmente.
    
    Args:
        start_date (str): Data de in√≠cio no formato YYYY-MM
        end_date (str): Data de fim no formato YYYY-MM
    
    Returns:
        List[Tuple[int, int]]: Lista de tuplas contendo (ano, m√™s) para cada m√™s
                               no intervalo especificado
    
    Examples:
        >>> generate_month_range("2023-01", "2023-03")
        [(2023, 1), (2023, 2), (2023, 3)]
        
        >>> generate_month_range("2022-11", "2023-02")
        [(2022, 11), (2022, 12), (2023, 1), (2023, 2)]
    """
    start, end = validar_datas(start_date, end_date)

    current = start
    months = []

    while current <= end:
        months.append((current.year, current.month))
        current += relativedelta(months=1)

    return months

# COMMAND ----------

def get_expected_schema() -> StructType:
    """
    Define o schema esperado para a tabela raw de dados de t√°xi.
    
    Retorna a estrutura de dados padronizada que deve ser seguida na camada Raw,
    garantindo consist√™ncia e facilitando valida√ß√µes.
    
    Returns:
        StructType: Schema do PySpark contendo todos os campos esperados com seus
                   tipos de dados correspondentes
    
    Examples:
        >>> schema = get_expected_schema()
        >>> print(schema.fieldNames())
        ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', ...]
    
    Notes:
        - Todos os campos s√£o nullable (True) para tolerar dados ausentes
        - Campos num√©ricos decimais s√£o DoubleType para m√°xima precis√£o
        - IDs s√£o LongType para suportar grandes valores
        - Datas/horas usam TimestampType e DateType nativos do Spark
    """
    return StructType([
        StructField("VendorID", LongType(), True),
        StructField("tpep_pickup_datetime", TimestampType(), True),
        StructField("tpep_dropoff_datetime", TimestampType(), True),
        StructField("passenger_count", DoubleType(), True),
        StructField("trip_distance", DoubleType(), True),
        StructField("RatecodeID", DoubleType(), True),
        StructField("store_and_fwd_flag", StringType(), True),
        StructField("PULocationID", LongType(), True),
        StructField("DOLocationID", LongType(), True),
        StructField("payment_type", LongType(), True),
        StructField("fare_amount", DoubleType(), True),
        StructField("extra", DoubleType(), True),
        StructField("mta_tax", DoubleType(), True),
        StructField("tip_amount", DoubleType(), True),
        StructField("tolls_amount", DoubleType(), True),
        StructField("improvement_surcharge", DoubleType(), True),
        StructField("total_amount", DoubleType(), True),
        StructField("congestion_surcharge", DoubleType(), True),
        StructField("airport_fee", DoubleType(), True),
        StructField("date_reference", DateType(), True),
        StructField("insertion_date", DateType(), True),
    ])

# COMMAND ----------

def check_columns(df: DataFrame, expected_schema: StructType) -> Tuple[Set[str], Set[str], Set[str]]:
    """
    Compara as colunas do DataFrame com o schema esperado.
    
    Identifica discrep√¢ncias entre as colunas presentes no DataFrame e as
    colunas definidas no schema esperado, facilitando diagn√≥stico de problemas.
    
    Args:
        df (DataFrame): DataFrame do PySpark a ser validado
        expected_schema (StructType): Schema esperado contendo defini√ß√£o de campos
    
    Returns:
        Tuple[Set[str], Set[str], Set[str]]: Tupla contendo:
            - expected_columns: Conjunto de colunas esperadas
            - missing_columns: Colunas esperadas mas ausentes no DataFrame
            - extra_columns: Colunas presentes mas n√£o esperadas
    
    Examples:
        >>> df = spark.read.parquet("s3://bucket/data.parquet")
        >>> schema = get_expected_schema()
        >>> expected, missing, extra = check_columns(df, schema)
        >>> print(f"Faltando: {missing}")
        Faltando: {'airport_fee'}
        >>> print(f"Extras: {extra}")
        Extras: {'unknown_column'}
    
    Notes:
        - Compara√ß√£o √© case-sensitive
        - Retorna sets vazios se n√£o houver discrep√¢ncias
        - √ötil para diagn√≥stico antes de aplicar transforma√ß√µes
    """
    df_columns = set(df.columns)
    expected_columns = set([field.name for field in expected_schema])
    missing_columns = expected_columns - df_columns
    extra_columns = df_columns - expected_columns
    
    return expected_columns, missing_columns, extra_columns

# COMMAND ----------

def normalize_column_names(df: DataFrame, expected_columns: Set[str]) -> DataFrame:
    """
    Normaliza nomes de colunas ignorando diferen√ßas de case (mai√∫sculas/min√∫sculas).
    
    Renomeia colunas que existem com grafia diferente (ex: 'vendorid' -> 'VendorID')
    para corresponder ao schema esperado, garantindo consist√™ncia.
    
    Args:
        df (DataFrame): DataFrame com poss√≠veis varia√ß√µes de case nos nomes
        expected_columns (Set[str]): Conjunto de nomes de colunas esperados
    
    Returns:
        DataFrame: DataFrame com colunas renomeadas para o padr√£o esperado
    
    Examples:
        >>> df = spark.createDataFrame([(1,)], ['vendorid'])
        >>> expected = {'VendorID'}
        >>> df_normalized = normalize_column_names(df, expected)
        >>> df_normalized.columns
        ['VendorID']
    
    Notes:
        - Realiza apenas renomea√ß√£o, n√£o adiciona/remove colunas
        - Compara√ß√£o case-insensitive (lower())
        - Preserva dados, altera apenas metadados das colunas
    """
    for col_atual in df.columns:
        for col_esperada in expected_columns:
            if col_atual.lower() == col_esperada.lower() and col_atual != col_esperada:
                df = df.withColumnRenamed(col_atual, col_esperada)
                logger.info(f"Coluna renomeada: '{col_atual}' -> '{col_esperada}'")
    
    return df

# COMMAND ----------

def add_missing_columns(df: DataFrame, missing_columns: Set[str], expected_schema: StructType) -> DataFrame:
    """
    Adiciona colunas faltantes ao DataFrame com valores NULL.
    
    Cria colunas que est√£o no schema esperado mas ausentes no DataFrame,
    preenchendo-as com NULL e aplicando o tipo de dado correto.
    
    Args:
        df (DataFrame): DataFrame ao qual ser√£o adicionadas colunas
        missing_columns (Set[str]): Conjunto de nomes de colunas faltantes
        expected_schema (StructType): Schema contendo tipos de dados esperados
    
    Returns:
        DataFrame: DataFrame com todas as colunas necess√°rias (incluindo NULLs)
    
    Examples:
        >>> df = spark.createDataFrame([(1,)], ['VendorID'])
        >>> missing = {'airport_fee'}
        >>> schema = get_expected_schema()
        >>> df_complete = add_missing_columns(df, missing, schema)
        >>> 'airport_fee' in df_complete.columns
        True
    
    Notes:
        - Valores NULL permitem que o registro seja processado
        - Tipo de dado √© extra√≠do do expected_schema
        - √ötil quando schema da fonte muda ao longo do tempo
    """
    for missing_col in missing_columns:
        for field in expected_schema:
            if missing_col == field.name:
                df = df.withColumn(missing_col, lit(None).cast(field.dataType))
                logger.warning(f"Coluna faltante adicionada como NULL: '{missing_col}'")
    
    return df

# COMMAND ----------

def cast_columns_to_schema(df: DataFrame, expected_schema: StructType) -> DataFrame:
    """
    Converte todas as colunas do DataFrame para os tipos definidos no schema.
    
    Aplica cast expl√≠cito em cada coluna para garantir conformidade com o
    schema esperado, evitando erros de tipo em opera√ß√µes posteriores.
    
    Args:
        df (DataFrame): DataFrame com poss√≠veis tipos incorretos
        expected_schema (StructType): Schema definindo tipos corretos para cada campo
    
    Returns:
        DataFrame: DataFrame com todas as colunas nos tipos esperados
    
    Examples:
        >>> df = spark.createDataFrame([(1, "2023-01-01")], ['VendorID', 'date'])
        >>> schema = get_expected_schema()
        >>> df_typed = cast_columns_to_schema(df, schema)
        >>> df_typed.schema['VendorID'].dataType
        LongType
    
    Notes:
        - Cast pode falhar silenciosamente gerando NULLs para valores inv√°lidos
        - Tipos s√£o aplicados na ordem do schema
        - Timestamps s√£o convertidos considerando timezone
    """
    for field in expected_schema:
        if field.name in df.columns:
            df = df.withColumn(field.name, col(field.name).cast(field.dataType))
    
    return df

# COMMAND ----------

def reorder_columns(df: DataFrame, expected_schema: StructType) -> DataFrame:
    """
    Reordena as colunas do DataFrame conforme a ordem do schema esperado.
    
    Garante que as colunas estejam na ordem padr√£o definida no schema,
    facilitando compara√ß√µes e joins futuros.
    
    Args:
        df (DataFrame): DataFrame com colunas em ordem arbitr√°ria
        expected_schema (StructType): Schema definindo a ordem esperada
    
    Returns:
        DataFrame: DataFrame com colunas na ordem do schema
    
    Examples:
        >>> df = spark.createDataFrame([(1, "A")], ['col2', 'col1'])
        >>> schema = StructType([StructField('col1', StringType()), 
        ...                      StructField('col2', LongType())])
        >>> df_ordered = reorder_columns(df, schema)
        >>> df_ordered.columns
        ['col1', 'col2']
    
    Notes:
        - Apenas reordena, n√£o adiciona/remove colunas
        - Colunas ausentes do schema s√£o ignoradas
        - Melhora legibilidade e previsibilidade dos dados
    """
    column_order = [field.name for field in expected_schema]
    return df.select(*column_order)

# COMMAND ----------

def validate_dataframe(df: DataFrame) -> None:
    """
    Executa valida√ß√µes b√°sicas de qualidade no DataFrame.
    
    Verifica se o DataFrame n√£o est√° vazio ap√≥s as transforma√ß√µes,
    levantando exce√ß√£o em caso de falha.
    
    Args:
        df (DataFrame): DataFrame a ser validado
    
    Raises:
        AssertionError: Se o DataFrame estiver vazio
    
    Examples:
        >>> df = spark.createDataFrame([(1,)], ['id'])
        >>> validate_dataframe(df)  # Passa sem erro
        
        >>> df_empty = spark.createDataFrame([], StructType([]))
        >>> validate_dataframe(df_empty)  # Levanta AssertionError
    
    Notes:
        - Valida√ß√£o m√≠nima, pode ser expandida com regras de neg√≥cio
        - isEmpty() for√ßa execu√ß√£o lazy do Spark
        - √ötil para detectar problemas antes de persistir dados
    """
    assert not df.isEmpty(), "DataFrame est√° vazio ap√≥s transforma√ß√µes"
    logger.info("‚úÖ DataFrame validado com sucesso")

# COMMAND ----------

def treat_and_test(df: DataFrame, expected_schema: StructType) -> DataFrame:
    """
    Aplica tratamentos e valida√ß√µes completas no DataFrame.
    
    Orquestra todas as etapas de normaliza√ß√£o, adi√ß√£o de colunas faltantes,
    convers√£o de tipos e valida√ß√£o, retornando um DataFrame pronto para persist√™ncia.
    
    Args:
        df (DataFrame): DataFrame bruto lido da landing zone
        expected_schema (StructType): Schema esperado para a camada raw
    
    Returns:
        DataFrame: DataFrame tratado e validado, conforme schema esperado
    
    Raises:
        AssertionError: Se houver colunas extras n√£o esperadas ou DataFrame vazio
    
    Examples:
        >>> df_raw = spark.read.parquet("s3://landing/yellow_tripdata_2023-01.parquet")
        >>> schema = get_expected_schema()
        >>> df_clean = treat_and_test(df_raw, schema)
        >>> df_clean.columns == [f.name for f in schema]
        True
    
    Side Effects:
        - Registra logs de cada etapa do tratamento
        - Pode modificar o DataFrame in-place (transforma√ß√µes Spark)
    
    Notes:
        - Pipeline completo: verificar -> normalizar -> adicionar -> converter -> ordenar -> validar
        - Falha r√°pido em caso de colunas extras inesperadas
        - Recomendado para garantir qualidade antes de persistir
    """
    logger.info("Iniciando tratamento e valida√ß√£o do DataFrame")
    
    # 1. Verificar colunas
    expected_columns, missing_columns, extra_columns = check_columns(df, expected_schema)
    
    # 2. Normalizar nomes (case-insensitive)
    df = normalize_column_names(df, expected_columns)
    
    # 3. Re-verificar ap√≥s normaliza√ß√£o
    expected_columns, missing_columns, extra_columns = check_columns(df, expected_schema)
    
    # 4. Adicionar colunas faltantes
    if missing_columns:
        logger.warning(f"Colunas faltantes detectadas: {missing_columns}")
        df = add_missing_columns(df, missing_columns, expected_schema)
    
    # 5. Converter tipos
    df = cast_columns_to_schema(df, expected_schema)
    
    # 6. Reordenar colunas
    df = reorder_columns(df, expected_schema)
    
    # 7. Valida√ß√£o final
    expected_columns, missing_columns, extra_columns = check_columns(df, expected_schema)
    
    assert not missing_columns, f"‚ùå Colunas ainda faltando ap√≥s tratamento: {missing_columns}"
    assert not extra_columns, f"‚ùå Colunas extras n√£o esperadas: {extra_columns}"
    
    validate_dataframe(df)
    
    logger.success("‚úÖ Tratamento e valida√ß√£o conclu√≠dos com sucesso")
    
    return df

# COMMAND ----------

def process_month_data(
    year: int, 
    month: int, 
    landing_path: str,
    catalogo: str,
    schema: str,
    table: str
) -> None:
    """
    Processa dados de um m√™s espec√≠fico da landing para a raw layer.
    
    Executa o pipeline completo para um √∫nico m√™s: leitura do Parquet na landing,
    adi√ß√£o de metadados, tratamento/valida√ß√£o, e persist√™ncia na tabela Delta raw.
    
    Args:
        year (int): Ano de refer√™ncia dos dados (ex: 2023)
        month (int): M√™s de refer√™ncia dos dados (1-12)
        landing_path (str): Caminho base da landing zone (ex: "s3://datalake-ifood/landing_layer")
        catalogo (str): Nome do cat√°logo Unity Catalog
        schema (str): Nome do schema/database
        table (str): Nome da tabela de destino
    
    Returns:
        None
    
    Raises:
        Exception: Se houver erro na leitura, transforma√ß√£o ou escrita dos dados
    
    Side Effects:
        - L√™ arquivo Parquet do S3
        - Deleta dados existentes da data de refer√™ncia (se houver)
        - Insere novos dados na tabela Delta
        - Registra logs de progresso e erros
    
    Examples:
        >>> process_month_data(2023, 5, "s3://datalake-ifood/landing_layer",
        ...                    "ifood_catalog", "raw_layer", "tb_taxi_data_api")
        # Dados de maio/2023 processados e persistidos
    
    Notes:
        - Opera√ß√£o √© idempotente: DELETE + INSERT garante n√£o duplica√ß√£o
        - date_reference marca o m√™s dos dados para particionamento l√≥gico
        - insertion_date registra quando os dados foram inseridos
        - Em caso de erro, dados parciais s√£o descartados (atomicidade do Delta)
    """
    date_base = f"{year}-{month:02d}"
    date_reference = f"{date_base}-01"
    
    logger.info(f"üîÑ Processando dados de: {date_base}")
    
    try:
        # Leitura do arquivo Parquet da landing zone
        file_path = f"{landing_path}/yellow/yellow_tripdata_{date_base}.parquet"
        logger.info(f"Lendo arquivo: {file_path}")
        
        df = spark.read.parquet(file_path)
        
        # Adicionar colunas de metadados
        df = df.withColumn(
            "tpep_pickup_datetime",
            col("tpep_pickup_datetime").cast("timestamp")
        ).withColumn(
            "tpep_dropoff_datetime",
            col("tpep_dropoff_datetime").cast("timestamp")
        ).withColumn(
            "date_reference",
            to_date(lit(date_reference), "yyyy-MM-dd")
        ).withColumn(
            "insertion_date",
            lit(now)
        )
        
        # Tratamento e valida√ß√£o
        expected_schema = get_expected_schema()
        df = treat_and_test(df, expected_schema)
        
        # Remover dados existentes da mesma data de refer√™ncia (idempot√™ncia)
        logger.info(f"Removendo dados existentes para date_reference = '{date_reference}'")
        spark.sql(f"""
            DELETE FROM {catalogo}.{schema}.{table}
            WHERE date_reference = '{date_reference}'
        """)
        
        # Persistir novos dados
        logger.info(f"Persistindo dados na tabela {catalogo}.{schema}.{table}")
        df.write.format("delta") \
            .mode("append") \
            .saveAsTable(f"{catalogo}.{schema}.{table}")
        
        logger.success(f"‚úÖ Dados de '{date_base}' persistidos com sucesso!")
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao processar '{date_base}': {str(e)}")
        raise

# COMMAND ----------

def run_pipeline(
    start_date: str, 
    end_date: str, 
    landing_path: str,
    catalogo: str,
    schema: str,
    table: str
) -> None:
    """
    Executa o pipeline completo de transforma√ß√£o Landing -> Raw para m√∫ltiplos meses.
    
    Orquestra o processamento sequencial de todos os meses no intervalo especificado,
    garantindo que a tabela existe antes de iniciar o processamento.
    
    Args:
        start_date (str): Data inicial no formato YYYY-MM (ex: "2023-01")
        end_date (str): Data final no formato YYYY-MM (ex: "2023-05")
        landing_path (str): Caminho base da landing zone
        catalogo (str): Nome do cat√°logo Unity Catalog
        schema (str): Nome do schema/database
        table (str): Nome da tabela de destino
    
    Returns:
        None
    
    Side Effects:
        - Cria tabela raw se n√£o existir
        - Processa m√∫ltiplos arquivos mensais
        - Persiste dados no Data Lake
        - Registra logs de execu√ß√£o
    
    Examples:
        >>> run_pipeline("2023-01", "2023-05", 
        ...              "s3://datalake-ifood/landing_layer",
        ...              "ifood_catalog", "raw_layer", "tb_taxi_data_api")
        # Processa jan-2023 at√© mai-2023
    
    Notes:
        - Processa meses sequencialmente (n√£o paralelo)
        - Falha em um m√™s n√£o interrompe processamento dos demais
        - Ideal para backfill ou processamento incremental
        - Opera√ß√£o idempotente: pode ser re-executada sem duplica√ß√£o
    """
    logger.info("=" * 80)
    logger.info("INICIANDO PIPELINE: Landing -> Raw Layer")
    logger.info("=" * 80)
    
    # Gerar lista de meses a processar
    months = generate_month_range(start_date, end_date)
    logger.info(f"Meses a processar: {len(months)} ({start_date} at√© {end_date})")
    
    # Processar cada m√™s
    for year, month in months:
        process_month_data(year, month, landing_path, catalogo, schema, table)
    
    logger.success("=" * 80)
    logger.success("PIPELINE CONCLU√çDO COM SUCESSO!")
    logger.success("=" * 80)

# COMMAND ----------

if __name__ == "__main__":
    """
    Ponto de entrada principal do notebook quando executado via Databricks Job.
    
    Widgets esperados:
        - start_date: Data in√≠cio no formato YYYY-MM
        - end_date: Data fim no formato YYYY-MM
        - landing_path: Caminho base da landing zone (ex: s3://datalake-ifood/landing_layer)
        - catalogo: Nome do cat√°logo (ex: ifood_catalog)
        - schema: Nome do schema (ex: raw_layer)
        - table: Nome da tabela (ex: tb_taxi_data_api)
    """
    start_date = dbutils.widgets.get("start_date")
    end_date = dbutils.widgets.get("end_date")
    landing_path = dbutils.widgets.get("landing_path")
    catalogo = dbutils.widgets.get("catalogo")
    schema = dbutils.widgets.get("schema")
    table = dbutils.widgets.get("table")
    
    run_pipeline(start_date, end_date, landing_path, catalogo, schema, table)

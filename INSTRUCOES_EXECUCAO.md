# ðŸ“– InstruÃ§Ãµes de ExecuÃ§Ã£o - Pipeline NYC Taxi Data

Este documento contÃ©m instruÃ§Ãµes detalhadas para executar o pipeline completo de ingestÃ£o e anÃ¡lise de dados de tÃ¡xi de Nova York.

## ðŸ“‘ Ãndice

- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [ConfiguraÃ§Ã£o Inicial](#-configuraÃ§Ã£o-inicial)
- [ExecuÃ§Ã£o Passo a Passo](#-execuÃ§Ã£o-passo-a-passo)
- [ExecuÃ§Ã£o via Databricks Workflow](#-execuÃ§Ã£o-via-databricks-workflow)
- [Troubleshooting](#-troubleshooting)
- [ValidaÃ§Ã£o dos Resultados](#-validaÃ§Ã£o-dos-resultados)

---

## ðŸ”§ PrÃ©-requisitos

### 1. Ambiente Databricks

- âœ… Conta no **Databricks Community Edition** (ou workspace corporativo)
- âœ… Cluster configurado com:
  - Runtime: **DBR 13.3 LTS ou superior**
  - Python: **3.10+**
  - Spark: **3.4+**

### 2. Armazenamento S3

- âœ… Bucket S3 criado (ex: `s3://datalake-ifood/`)
- âœ… Credenciais AWS com permissÃµes de leitura/escrita:
  - `aws_access_key_id`
  - `aws_secret_access_key`

### 3. Unity Catalog (Recomendado)

- âœ… CatÃ¡logo criado (ex: `ifood_catalog`)
- âœ… Schemas criados:
  - `raw_layer`
  - `trusted_layer`

### 4. Bibliotecas Python

Todas as dependÃªncias sÃ£o instaladas automaticamente via `%pip install` nos notebooks.

---

## âš™ï¸ ConfiguraÃ§Ã£o Inicial

### Passo 1: Criar Estrutura do Data Lake no S3

```bash
# Estrutura de diretÃ³rios no S3
s3://datalake-ifood/
â”œâ”€â”€ landing_layer/
â”‚   â””â”€â”€ yellow/              # Arquivos Parquet originais
â”œâ”€â”€ raw_layer/
â”‚   â””â”€â”€ tb_taxi_data_api/    # Dados brutos (Delta)
â””â”€â”€ trusted_layer/
    â””â”€â”€ tb_taxi_data_for_analysis/  # Dados refinados (Delta)
```

### Passo 2: Configurar Unity Catalog

Execute no SQL Editor do Databricks:

```sql
-- Criar catÃ¡logo (se nÃ£o existir)
CREATE CATALOG IF NOT EXISTS ifood_catalog;

-- Criar schemas com localizaÃ§Ã£o externa
CREATE SCHEMA IF NOT EXISTS ifood_catalog.raw_layer
COMMENT 'Camada raw - dados brutos'
MANAGED LOCATION 's3://datalake-ifood/raw_layer/';

CREATE SCHEMA IF NOT EXISTS ifood_catalog.trusted_layer
COMMENT 'Camada trusted - dados refinados'
MANAGED LOCATION 's3://datalake-ifood/trusted_layer/';
```

### Passo 3: Configurar Secrets (Recomendado)

Para seguranÃ§a, armazene credenciais AWS no Databricks Secrets:

```python
# Via CLI do Databricks
databricks secrets create-scope --scope aws-credentials
databricks secrets put --scope aws-credentials --key access-key-id
databricks secrets put --scope aws-credentials --key secret-access-key
```

---

## ðŸš€ ExecuÃ§Ã£o Passo a Passo

### Etapa 1: Extrair Dados para Landing Zone

**Arquivo:** `src/extract_to_landing.py`

#### ExecuÃ§Ã£o Manual no Notebook:

1. Abra o notebook no Databricks
2. Configure os widgets (parÃ¢metros):

```python
# Criar widgets
dbutils.widgets.text("datalake_path", "s3://datalake-ifood/landing_layer")
dbutils.widgets.text("start_date", "2023-01")
dbutils.widgets.text("end_date", "2023-05")
dbutils.widgets.text("aws_access_key_id", "AKIA...")
dbutils.widgets.text("aws_secret_access_key", "******")
```

3. Execute todas as cÃ©lulas (`Run All`)

#### O que acontece:

- âœ… Download de arquivos Parquet da NYC TLC (Jan-Mai 2023)
- âœ… Upload para `s3://datalake-ifood/landing_layer/yellow/`
- âœ… VerificaÃ§Ã£o de duplicatas (idempotente)

#### Tempo estimado: **30 segundos**

---

### Etapa 2: Transformar para Raw Layer

**Arquivo:** `src/landing_to_raw.py`

#### ExecuÃ§Ã£o Manual no Notebook:

1. Abra o notebook no Databricks
2. Configure os widgets:

```python
dbutils.widgets.text("start_date", "2023-01")
dbutils.widgets.text("end_date", "2023-05")
dbutils.widgets.text("landing_path", "s3://datalake-ifood/landing_layer")
dbutils.widgets.text("catalogo", "ifood_catalog")
dbutils.widgets.text("schema", "raw_layer")
dbutils.widgets.text("table", "tb_taxi_data_api")
```

3. Execute todas as cÃ©lulas (`Run All`)

#### O que acontece:

- âœ… Leitura dos Parquets da landing
- âœ… ValidaÃ§Ã£o e normalizaÃ§Ã£o de schema
- âœ… CriaÃ§Ã£o da tabela Delta (se nÃ£o existir)
- âœ… InserÃ§Ã£o de dados com DELETE + INSERT (idempotente)
- âœ… Clustering por `date_reference` e `VendorID`

#### Tempo estimado: **2 minutos**

#### VerificaÃ§Ã£o:

```sql
SELECT COUNT(*) AS total_registros 
FROM ifood_catalog.raw_layer.tb_taxi_data_api;

-- Resultado esperado: ~14 milhÃµes de registros (Jan-Mai 2023)
```

---

### Etapa 3: Refinar para Trusted Layer

**Arquivo:** `src/raw_to_trusted.sql`

#### ExecuÃ§Ã£o Manual no Notebook:

1. Abra o notebook SQL no Databricks
2. Configure os widgets:

```sql
-- No inÃ­cio do notebook
CREATE WIDGET TEXT catalogo DEFAULT 'ifood_catalog';
CREATE WIDGET TEXT schema DEFAULT 'trusted_layer';
CREATE WIDGET TEXT table DEFAULT 'tb_taxi_data_for_analysis';
```

3. Execute todas as cÃ©lulas (`Run All`)

#### O que acontece:

- âœ… CriaÃ§Ã£o da tabela trusted (se nÃ£o existir)
- âœ… MERGE com SELECT DISTINCT da raw
- âœ… MantÃ©m apenas colunas necessÃ¡rias:
  - `vendor_id`
  - `tpep_pickup_datetime`
  - `tpep_dropoff_datetime`
  - `passenger_count`
  - `total_amount`
- âœ… Remove duplicatas
- âœ… Clustering por `vendor_id`

#### Tempo estimado: **30 segundos**

#### VerificaÃ§Ã£o:

```sql
SELECT 
  COUNT(*) AS total_registros,
  COUNT(DISTINCT vendor_id) AS vendors_unicos,
  MIN(tpep_pickup_datetime) AS primeira_corrida,
  MAX(tpep_pickup_datetime) AS ultima_corrida
FROM ifood_catalog.trusted_layer.tb_taxi_data_for_analysis;
```

---

### Etapa 4: Executar AnÃ¡lises

**Arquivos:** 
- `analysis/analysis_queries.sql` (SQL)
- `analysis/analysis_queries.py` (PySpark - alternativa)

#### OpÃ§Ã£o A: AnÃ¡lises em SQL

1. Abra `analysis_queries.sql` no Databricks
2. Execute as cÃ©lulas sequencialmente
3. Visualize os resultados inline

**Principais queries:**

```sql
-- Pergunta 1: MÃ©dia de valor total por mÃªs
SELECT 
    YEAR(tpep_pickup_datetime) AS ano,
    MONTH(tpep_pickup_datetime) AS mes,
    ROUND(AVG(total_amount), 2) AS media_valor_total,
    COUNT(*) AS total_corridas
FROM ifood_catalog.trusted_layer.tb_taxi_data_for_analysis
WHERE total_amount > 0
GROUP BY YEAR(tpep_pickup_datetime), MONTH(tpep_pickup_datetime)
ORDER BY ano, mes;

-- Pergunta 2: MÃ©dia de passageiros por hora (Maio/2023)
SELECT 
    HOUR(tpep_pickup_datetime) AS hora_do_dia,
    ROUND(AVG(passenger_count), 2) AS media_passageiros,
    COUNT(*) AS total_corridas
FROM ifood_catalog.trusted_layer.tb_taxi_data_for_analysis
WHERE YEAR(tpep_pickup_datetime) = 2023
    AND MONTH(tpep_pickup_datetime) = 5
    AND passenger_count > 0
GROUP BY HOUR(tpep_pickup_datetime)
ORDER BY hora_do_dia;
```

#### OpÃ§Ã£o B: AnÃ¡lises em PySpark

1. Abra `analysis_queries.py` no Databricks
2. Execute todas as cÃ©lulas
3. Resultados sÃ£o exibidos via `.show()` e logs

#### Tempo estimado: **2-5 minutos**

---

## ðŸ”„ ExecuÃ§Ã£o via Databricks Workflow

### Criar Workflow Automatizado

1. Navegue para **Workflows** â†’ **Create Job**

2. Configure as tasks na ordem:

#### Task 1: Extract to Landing
```yaml
Task name: extract_to_landing
Type: Notebook
Path: /Workspace/ifood-case/src/extract_to_landing
Cluster: Cluster compartilhado
Parameters:
  - datalake_path: s3://datalake-ifood/landing_layer
  - start_date: 2023-01
  - end_date: 2023-05
  - aws_access_key_id: {{secrets/aws-credentials/access-key-id}}
  - aws_secret_access_key: {{secrets/aws-credentials/secret-access-key}}
```

#### Task 2: Landing to Raw
```yaml
Task name: landing_to_raw
Type: Notebook
Path: /Workspace/ifood-case/src/landing_to_raw
Depends on: extract_to_landing
Cluster: Cluster compartilhado
Parameters:
  - start_date: 2023-01
  - end_date: 2023-05
  - landing_path: s3://datalake-ifood/landing_layer
  - catalogo: ifood_catalog
  - schema: raw_layer
  - table: tb_taxi_data_api
```

#### Task 3: Raw to Trusted
```yaml
Task name: raw_to_trusted
Type: Notebook
Path: /Workspace/ifood-case/src/raw_to_trusted
Depends on: landing_to_raw
Cluster: Cluster compartilhado
Parameters:
  - catalogo: ifood_catalog
  - schema: trusted_layer
  - table: tb_taxi_data_for_analysis
```

#### Task 4: Run Analysis
```yaml
Task name: run_analysis
Type: Notebook
Path: /Workspace/ifood-case/analysis/analysis_queries
Depends on: raw_to_trusted
Cluster: Cluster compartilhado
```

3. Configure Schedule (opcional):
```yaml
Schedule: Cron
Expression: 0 2 * * * (diariamente Ã s 2h AM)
Timezone: America/Sao_Paulo
```

4. **Run Now** para executar manualmente

---

## ðŸ› Troubleshooting

### Problema 1: Erro de Credenciais AWS

**Sintoma:**
```
ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden
```

**SoluÃ§Ã£o:**
- Verificar se as credenciais AWS estÃ£o corretas
- Confirmar permissÃµes IAM: `s3:GetObject`, `s3:PutObject`, `s3:ListBucket`
- Validar se o bucket existe

---

### Problema 2: Tabela nÃ£o encontrada

**Sintoma:**
```
[TABLE_OR_VIEW_NOT_FOUND] The table or view `ifood_catalog`.`raw_layer`.`tb_taxi_data_api` cannot be found
```

**SoluÃ§Ã£o:**
```sql
-- Verificar se o catÃ¡logo existe
SHOW CATALOGS;

-- Verificar se o schema existe
SHOW SCHEMAS IN ifood_catalog;

-- Recriar schema se necessÃ¡rio
CREATE SCHEMA IF NOT EXISTS ifood_catalog.raw_layer
MANAGED LOCATION 's3://datalake-ifood/raw_layer/';
```

---

### Problema 3: Arquivo Parquet nÃ£o encontrado

**Sintoma:**
```
Path does not exist: s3://datalake-ifood/landing_layer/yellow/yellow_tripdata_2023-01.parquet
```

**SoluÃ§Ã£o:**
- Re-executar `extract_to_landing.py`
- Verificar se o download foi concluÃ­do com sucesso
- Validar se o caminho do S3 estÃ¡ correto

---

### Problema 4: Duplicatas na Trusted

**Sintoma:**
```
Registros duplicados detectados apÃ³s MERGE
```

**SoluÃ§Ã£o:**
```sql
-- Limpar a tabela e re-executar o MERGE
TRUNCATE TABLE ifood_catalog.trusted_layer.tb_taxi_data_for_analysis;

-- Re-executar raw_to_trusted.sql
```

---

### Problema 5: Performance lenta

**Sintoma:**
- Jobs demoram mais de 30 minutos

**SoluÃ§Ã£o:**
1. Aumentar tamanho do cluster (mais workers)
2. Habilitar Photon:
```python
spark.conf.set("spark.databricks.photon.enabled", "true")
```
3. Verificar clustering das tabelas:
```sql
DESCRIBE DETAIL ifood_catalog.raw_layer.tb_taxi_data_api;
```

---

## âœ… ValidaÃ§Ã£o dos Resultados

### Checklist Final

Execute as queries abaixo para validar a execuÃ§Ã£o completa:

#### 1. Landing Layer
```bash
# Via AWS CLI
aws s3 ls s3://datalake-ifood/landing_layer/yellow/ --recursive

# Deve listar 5 arquivos:
# yellow_tripdata_2023-01.parquet
# yellow_tripdata_2023-02.parquet
# yellow_tripdata_2023-03.parquet
# yellow_tripdata_2023-04.parquet
# yellow_tripdata_2023-05.parquet
```

#### 2. Raw Layer
```sql
SELECT 
  'Total de Registros' AS metrica,
  CAST(COUNT(*) AS STRING) AS valor
FROM ifood_catalog.raw_layer.tb_taxi_data_api

UNION ALL

SELECT 
  'PerÃ­odo Coberto' AS metrica,
  CONCAT(
    CAST(MIN(date_reference) AS STRING), 
    ' atÃ© ', 
    CAST(MAX(date_reference) AS STRING)
  ) AS valor
FROM ifood_catalog.raw_layer.tb_taxi_data_api;

-- Resultado esperado:
-- Total de Registros: ~16.186.386
-- PerÃ­odo Coberto: 2023-01-01 atÃ© 2023-05-01
```

#### 3. Trusted Layer
```sql
SELECT 
  COUNT(*) AS total_registros,
  COUNT(DISTINCT vendor_id) AS vendors_unicos,
  ROUND(AVG(total_amount), 2) AS media_valor,
  ROUND(AVG(passenger_count), 2) AS media_passageiros
FROM ifood_catalog.trusted_layer.tb_taxi_data_for_analysis
WHERE total_amount > 0 AND passenger_count > 0;

-- Resultado esperado:
-- total_registros: ~15.340.356
-- vendors_unicos: 2-4
-- media_valor: ~28.32
-- media_passageiros: ~1.39
```

#### 4. AnÃ¡lises - Pergunta 1
```sql
SELECT 
    YEAR(tpep_pickup_datetime) AS ano,
    MONTH(tpep_pickup_datetime) AS mes,
    CONCAT(
        YEAR(tpep_pickup_datetime), 
        '-', 
        LPAD(MONTH(tpep_pickup_datetime), 2, '0')
    ) AS periodo,
    ROUND(AVG(total_amount), 2) AS media_valor_total
FROM ifood_catalog.trusted_layer.tb_taxi_data_for_analysis
WHERE total_amount IS NOT NULL
    AND total_amount > 0
    AND YEAR(tpep_pickup_datetime) = 2023
    AND MONTH(tpep_pickup_datetime) <= 5
GROUP BY 
    YEAR(tpep_pickup_datetime),
    MONTH(tpep_pickup_datetime)
ORDER BY ano, mes;

-- Resultado esperado: 5 linhas (2023-01 atÃ© 2023-05)
-- Valores entre $27-30
```

#### 5. AnÃ¡lises - Pergunta 2
```sql
SELECT 
    HOUR(tpep_pickup_datetime) AS hora_do_dia,
    ROUND(AVG(passenger_count), 2) AS media_passageiros
FROM ifood_catalog.trusted_layer.tb_taxi_data_for_analysis
WHERE YEAR(tpep_pickup_datetime) = 2023
    AND MONTH(tpep_pickup_datetime) = 5
    AND passenger_count IS NOT NULL
    AND passenger_count > 0
GROUP BY HOUR(tpep_pickup_datetime)
ORDER BY hora_do_dia;

-- Resultado esperado: 24 linhas (hora 0-23)
-- Valores entre 1.26-1.45
```

---

## ðŸ“Š Exportar Resultados

### OpÃ§Ã£o 1: Via Databricks UI

1. Execute a query de anÃ¡lise
2. Clique em **Download** â†’ **CSV**
3. Salve localmente

### OpÃ§Ã£o 2: Via CÃ³digo

```python
# Exportar para CSV no DBFS
df = spark.table("ifood_catalog.trusted_layer.tb_taxi_data_for_analysis")
df.coalesce(1).write.csv("dbfs:/FileStore/resultados/taxi_data.csv", header=True)

# Download via UI: Data â†’ DBFS â†’ FileStore â†’ resultados
```

### OpÃ§Ã£o 3: Para S3

```python
df.write.mode("overwrite").parquet("s3://datalake-ifood/exports/taxi_analysis/")
```

---

## ðŸŽ¯ PrÃ³ximos Passos

ApÃ³s validaÃ§Ã£o bem-sucedida:

1. âœ… **Conectar ferramentas de BI** (Tableau, Power BI, Looker)
2. âœ… **Criar dashboards** com as anÃ¡lises
3. âœ… **Configurar alertas** para anomalias
4. âœ… **Expandir para mais meses** alterando `start_date` e `end_date`
5. âœ… **Adicionar testes automatizados** (Great Expectations, dbt tests)
6. âœ… **Implementar CI/CD** para deploy automÃ¡tico

---

## ðŸ“ž Suporte

Em caso de dÃºvidas:

- ðŸ“§ Abra uma **issue** no repositÃ³rio GitHub
- ðŸ“– Consulte a [documentaÃ§Ã£o oficial do Databricks](https://docs.databricks.com/)
- ðŸ“– Consulte o [Data Dictionary da NYC TLC](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)

---

**Autor:** Vinicius  
**Data:** 2025  
**VersÃ£o:** 1.0